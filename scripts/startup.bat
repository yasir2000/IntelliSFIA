@echo off
REM IntelliSFIA AI Framework Startup Script for Windows
REM ===================================================
REM 
REM This script starts the complete IntelliSFIA AI Framework:
REM 1. FastAPI backend with CrewAI multi-agent system
REM 2. React frontend with enhanced AI integration
REM 3. Ollama service for local LLM processing

setlocal enabledelayedexpansion

echo ðŸš€ Starting IntelliSFIA AI Framework...
echo ======================================

REM Check if Python is installed
python --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Python is not installed. Please install Python 3.8+ and try again.
    pause
    exit /b 1
)

REM Check if Node.js is installed
node --version >nul 2>&1
if errorlevel 1 (
    echo âŒ Node.js is not installed. Please install Node.js and try again.
    pause
    exit /b 1
)

REM Check if Ollama is installed
ollama --version >nul 2>&1
if errorlevel 1 (
    echo âš ï¸  Ollama is not installed. Please install Ollama for local LLM support.
    echo    Visit: https://ollama.ai
)

echo ðŸ“¦ Setting up Python environment...

REM Install Python dependencies
if exist requirements.txt (
    echo Installing Python dependencies...
    pip install -r requirements.txt
) else (
    echo Installing core Python dependencies...
    pip install fastapi uvicorn pydantic crewai rdflib sparqlwrapper python-multipart
)

echo âœ“ Python environment ready

echo ðŸ“¦ Setting up Node.js environment...

REM Navigate to frontend directory and install dependencies
if exist "sfia_ai_framework\frontend" (
    cd sfia_ai_framework\frontend
    
    if exist package.json (
        echo Installing Node.js dependencies...
        call npm install
    ) else (
        echo âš ï¸  package.json not found in frontend directory
    )
    
    cd ..\..
) else (
    echo âš ï¸  Frontend directory not found
)

echo âœ“ Node.js environment ready

REM Start Ollama service if available
ollama --version >nul 2>&1
if not errorlevel 1 (
    echo ðŸ¤– Starting Ollama service...
    start /B ollama serve
    timeout /t 3 /nobreak >nul
    
    REM Check for DeepSeek model
    echo Checking for DeepSeek model...
    ollama list | findstr "deepseek-coder" >nul 2>&1
    if errorlevel 1 (
        echo Pulling DeepSeek Coder model (this may take a while)...
        ollama pull deepseek-coder:6.7b
    )
    
    echo âœ“ Ollama service ready
)

echo ðŸš€ Starting FastAPI Backend...

REM Start the FastAPI backend
if exist intellisfia_ai_api.py (
    start /B python intellisfia_ai_api.py
    echo âœ“ Backend started on http://localhost:8000
) else (
    echo âŒ intellisfia_ai_api.py not found
    pause
    exit /b 1
)

REM Wait for backend to start
timeout /t 5 /nobreak >nul

echo ðŸŒ Starting React Frontend...

REM Start the React frontend
if exist "sfia_ai_framework\frontend" (
    cd sfia_ai_framework\frontend
    start /B npm start
    cd ..\..
    echo âœ“ Frontend started on http://localhost:3000
) else (
    echo âŒ Frontend directory not found
    pause
    exit /b 1
)

echo.
echo ðŸŽ‰ IntelliSFIA AI Framework is now running!
echo =============================================
echo ðŸ“Š Frontend:      http://localhost:3000
echo ðŸ”§ Backend API:   http://localhost:8000
echo ðŸ“š API Docs:      http://localhost:8000/docs
ollama --version >nul 2>&1
if not errorlevel 1 (
    echo ðŸ¤– Ollama:        http://localhost:11434
)
echo.
echo Features Available:
echo â€¢ CrewAI Multi-Agent System
echo â€¢ SFIA Semantic Ontology (RDF/OWL)
echo â€¢ Conversation Memory
echo â€¢ Evidence Validation
echo â€¢ AI-Powered Assessments
echo â€¢ Real-time Chat Interface
echo.
echo Press any key to open the application in your browser...
pause >nul

REM Open the application in the default browser
start http://localhost:3000

echo.
echo Press any key to stop all services...
pause >nul

REM Stop services (simplified - in real scenario you'd track PIDs)
taskkill /F /IM python.exe >nul 2>&1
taskkill /F /IM node.exe >nul 2>&1
taskkill /F /IM ollama.exe >nul 2>&1

echo Services stopped.
pause